# -*- coding: utf-8 -*-
"""GBM WITH Preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CRZAhXkD5GUP3ec0bsgkGlDo-I-7pQq1
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/ML PROJECT/data.csv')

# Preprocessing
# Remove unnecessary columns
df.drop(columns=['id', 'Unnamed: 32'], inplace=True, errors='ignore')

# Encode target variable
label_encoder = LabelEncoder()
df['diagnosis'] = label_encoder.fit_transform(df['diagnosis'])  # 0: Benign, 1: Malignant

# Split features and target variable
X = df.drop(columns=['diagnosis'])
y = df['diagnosis']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# Balance classes using Random Under Sampling
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)

# Remove highly correlated features
def remove_highly_correlated_features(df, threshold=0.9):
    correlation_matrix = df.corr()
    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > threshold)]
    return df.drop(columns=to_drop), to_drop

X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
X_resampled_cleaned, dropped_features = remove_highly_correlated_features(X_resampled_df, threshold=0.9)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled_cleaned, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Define Stratified K-Fold
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# GBM model before tuning
gbm_model = GradientBoostingClassifier(random_state=42)

# Cross-validation for baseline GBM model
cv_results = {
    "accuracy": cross_val_score(gbm_model, X_train, y_train, cv=kfold, scoring='accuracy'),
    "precision": cross_val_score(gbm_model, X_train, y_train, cv=kfold, scoring='precision'),
    "recall": cross_val_score(gbm_model, X_train, y_train, cv=kfold, scoring='recall'),
    "f1": cross_val_score(gbm_model, X_train, y_train, cv=kfold, scoring='f1'),
    "roc_auc": cross_val_score(gbm_model, X_train, y_train, cv=kfold, scoring='roc_auc'),
}

print("\nðŸ“Š Cross-Validation Results (Baseline GBM, mean Â± std):")
for metric, scores in cv_results.items():
    print(f"{metric.capitalize()}: {scores.mean():.4f} Â± {scores.std():.4f}")

# Fit and evaluate baseline model on test set
gbm_model.fit(X_train, y_train)
y_pred_baseline = gbm_model.predict(X_test)
baseline_metrics = {
    "accuracy": accuracy_score(y_test, y_pred_baseline),
    "precision": precision_score(y_test, y_pred_baseline),
    "recall": recall_score(y_test, y_pred_baseline),
    "f1": f1_score(y_test, y_pred_baseline),
    "roc_auc": roc_auc_score(y_test, gbm_model.predict_proba(X_test)[:, 1]),
}

print("\nðŸ“Š Final Model Evaluation (Baseline GBM, Test Set):")
for metric, score in baseline_metrics.items():
    print(f"{metric.capitalize()}: {score:.4f}")

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4],
    'min_samples_split': [2, 5]
}

grid_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid,
    cv=kfold,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# Best model after GridSearchCV
best_gbm_model = grid_search.best_estimator_

# Evaluate best model on test set
y_pred_best = best_gbm_model.predict(X_test)
best_metrics = {
    "accuracy": accuracy_score(y_test, y_pred_best),
    "precision": precision_score(y_test, y_pred_best),
    "recall": recall_score(y_test, y_pred_best),
    "f1": f1_score(y_test, y_pred_best),
    "roc_auc": roc_auc_score(y_test, best_gbm_model.predict_proba(X_test)[:, 1]),
}

print("\nðŸ“Š Best Model Evaluation (After Tuning, Test Set):")
for metric, score in best_metrics.items():
    print(f"{metric.capitalize()}: {score:.4f}")

# Confusion Matrix for Best Model
conf_matrix = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=["Benign", "Malignant"], yticklabels=["Benign", "Malignant"])
plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

# ROC Curve
y_pred_proba_best = best_gbm_model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_best)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, label=f"GBM Model (AUC = {auc(fpr, tpr):.4f})", color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Guess')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.legend(loc="lower right")
plt.grid(alpha=0.5)
plt.show()

# Combine metrics for visualization
metrics_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],
    'Baseline': list(baseline_metrics.values()),
    'Tuned': list(best_metrics.values())
})

# Bar plot to compare baseline and tuned metrics
metrics_df.set_index('Metric', inplace=True)
metrics_df.plot(kind='bar', figsize=(10, 6), color=['skyblue', 'orange'])
plt.title('Baseline vs Tuned Model Performance (Test Set)')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.legend(title='Model', loc='lower right')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()