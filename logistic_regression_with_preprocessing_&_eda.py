# -*- coding: utf-8 -*-
"""LOGISTIC REGRESSION WITH Preprocessing & EDA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18b5z5oU2aOwXaQu_9LVuhs_83yylCCVP
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.under_sampling import RandomUnderSampler

# Load the original dataset
df_original = pd.read_csv('/content/drive/MyDrive/ML PROJECT/data.csv')

# Display the first 5 rows
print("ðŸ“Š First 5 Rows:")
print(df_original.head())

# Dataset Information
print("\nðŸ“Š Dataset Information:")
print(df_original.info())

# Dataset Shape
print("\nðŸ“Š Dataset Shape:")
print(df_original.shape)

# Removing Unnecessary Columns
df_original.drop(columns=['id', 'Unnamed: 32'], inplace=True, errors='ignore')

# Check for missing values
missing_values = df_original.isnull().sum()
print("\nðŸ“Š Missing Values:\n", missing_values)
if missing_values.any():
    df_original.fillna(df_original.median(), inplace=True)

# Encoding Target Variable
label_encoder = LabelEncoder()
df_original['diagnosis'] = label_encoder.fit_transform(df_original['diagnosis'])  # 0: Benign, 1: Malignant

# ðŸ”„ Splitting Features and Target Variable
x = df_original.drop(columns=['diagnosis'])
y = df_original['diagnosis']

# ðŸ“Š Target Variable Distribution
print("\nðŸ“Š Target Variable Distribution (Before):\n", y.value_counts())
sns.countplot(x=y, palette='viridis')
plt.title('Class Distribution (0: Benign, 1: Malignant)')
plt.show()

# Scaling features
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)
x = pd.DataFrame(x_scaled, columns=x.columns)

# Applying Random Under Sampling
rus = RandomUnderSampler(random_state=42)
x_resampled, y_resampled = rus.fit_resample(x, y)

# Display balanced class distribution
print("\nðŸ“Š Balanced Class Distribution:\n", y_resampled.value_counts())
sns.countplot(x=y_resampled, palette='husl')
plt.title('Balanced Class Distribution (After RUS)')
plt.show()

# High correlation feature removal
def remove_highly_correlated_features(df, threshold=0.9):
    correlation_matrix = df.corr()
    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > threshold)]
    return df.drop(columns=to_drop), to_drop

# Remove highly correlated features
x_resampled_df = pd.DataFrame(x_resampled, columns=x.columns)
x_resampled_cleaned, dropped_features = remove_highly_correlated_features(x_resampled_df, threshold=0.9)

print(f"\nðŸ“Š Dropped Features (Correlation > 0.9): {dropped_features}")

# ðŸ“ˆ Scatter Plot: Relationship Between Two Features
df_resampled = pd.concat([x_resampled_cleaned, pd.Series(y_resampled, name='diagnosis')], axis=1)
sns.scatterplot(x='radius_mean', y='texture_mean', hue='diagnosis', palette='husl', data=df_resampled)
plt.legend(bbox_to_anchor=(1, 1), loc=2)
plt.title('Radius Mean vs Texture Mean')
plt.show()

# ðŸ“Š Pair Plot (Selected Features)
selected_features = ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'diagnosis']
sns.pairplot(df_resampled[selected_features], hue='diagnosis', palette='husl', height=2)
plt.show()

# ðŸ“ˆ Histograms
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0,0].set_title("Radius Mean")
axes[0,0].hist(df_original['radius_mean'], bins=15, color='skyblue')

axes[0,1].set_title("Texture Mean")
axes[0,1].hist(df_original['texture_mean'], bins=15, color='salmon')

axes[1,0].set_title("Smoothness Mean")
axes[1,0].hist(df_original['smoothness_mean'], bins=15, color='lightgreen')

axes[1,1].set_title("Compactness Mean")
axes[1,1].hist(df_original['compactness_mean'], bins=15, color='orange')

plt.tight_layout()
plt.show()

# ðŸ“Š Distribution Plots (FacetGrid)
plot = sns.FacetGrid(df_original, hue="diagnosis", palette="husl")
plot.map(sns.kdeplot, "radius_mean").add_legend()
plot.map(sns.kdeplot, "texture_mean").add_legend()
plot.map(sns.kdeplot, "smoothness_mean").add_legend()
plot.map(sns.kdeplot, "compactness_mean").add_legend()
plt.show()

# ðŸ“Š Boxplot Graphs
def graph(y):
    sns.boxplot(x="diagnosis", y=y, data=df_resampled)

plt.figure(figsize=(12, 10))

plt.subplot(221)
graph('radius_mean')

plt.subplot(222)
graph('texture_mean')

plt.subplot(223)
graph('smoothness_mean')

plt.subplot(224)
graph('compactness_mean')

plt.tight_layout()
plt.show()

# ðŸ“Š Outlier Removal Function (By Class)
def remove_outliers_by_class(df, column, class_column):
    cleaned_df = pd.DataFrame()  # Create an empty DataFrame
    for cls in df[class_column].unique():
        class_subset = df[df[class_column] == cls]  # Get subset for the class
        Q1 = np.percentile(class_subset[column], 25)  # First quartile
        Q3 = np.percentile(class_subset[column], 75)  # Third quartile
        IQR = Q3 - Q1  # Interquartile range
        lower_bound = Q1 - 1.5 * IQR  # Lower bound
        upper_bound = Q3 + 1.5 * IQR  # Upper bound
        class_subset = class_subset[(class_subset[column] >= lower_bound) & (class_subset[column] <= upper_bound)]  # Remove outliers
        cleaned_df = pd.concat([cleaned_df, class_subset], axis=0)  # Append cleaned data
    return cleaned_df

# Using the Outlier Removal Function
df_resampled_cleaned = remove_outliers_by_class(df_resampled, 'radius_mean', 'diagnosis')

# Visualize boxplot after outlier removal
plt.figure(figsize=(12, 6))
sns.boxplot(x='diagnosis', y='radius_mean', data=df_resampled_cleaned)
plt.title('Boxplot of Radius Mean After Outlier Removal')
plt.show()

# Heatmap for Feature Correlation
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.figure(figsize=(15, 12))
correlation_matrix = df_original.corr()
sns.heatmap(correlation_matrix, annot=True, cmap=cmap, linewidths=0.5)
plt.title('Feature Correlation Heatmap (Initially)')
plt.show()

# Heatmap for Feature Correlation After Dropping
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.figure(figsize=(15, 12))
correlation_matrix = x_resampled_cleaned.corr()
sns.heatmap(correlation_matrix, annot=True, cmap=cmap, linewidths=0.5)
plt.title('Feature Correlation Heatmap (After Dropping)')
plt.show()

# ðŸ“Š Final Dataset Shape
print("\nðŸ“Š Final Dataset Shape:", x_resampled_cleaned.shape, y_resampled.shape)
print("\nðŸ“Š Feature Summary:\n", x_resampled_cleaned.describe())

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve, auc
)
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure y_resampled is a pandas Series
y_resampled = pd.Series(y_resampled)
x_resampled_df = pd.DataFrame(x_resampled, columns=x.columns)

# Stratified K-Fold cross-validation
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Define empty lists to store metrics
accuracy_scores, precision_scores, recall_scores, f1_scores, aucroc_scores = [], [], [], [], []

# K-Fold Cross-Validation
for train_index, val_index in cv.split(x_resampled_df, y_resampled):
    x_train, x_val = x_resampled_df.iloc[train_index], x_resampled_df.iloc[val_index]
    y_train, y_val = y_resampled.iloc[train_index], y_resampled.iloc[val_index]

    # Logistic Regression Model
    logistic_model = LogisticRegression(random_state=42)
    logistic_model.fit(x_train, y_train)
    y_pred = logistic_model.predict(x_val)

    # Evaluate metrics
    accuracy_scores.append(accuracy_score(y_val, y_pred))
    precision_scores.append(precision_score(y_val, y_pred))
    recall_scores.append(recall_score(y_val, y_pred))
    f1_scores.append(f1_score(y_val, y_pred))
    aucroc_scores.append(roc_auc_score(y_val, y_pred))

# Print cross-validation results
print("\nðŸ“Š Cross-Validation Results (mean Â± std):")
print(f"Accuracy: {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}")
print(f"Precision: {np.mean(precision_scores):.4f} Â± {np.std(precision_scores):.4f}")
print(f"Recall: {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}")
print(f"F1-Score: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")
print(f"AUC-ROC: {np.mean(aucroc_scores):.4f} Â± {np.std(aucroc_scores):.4f}")

# Train-Test Split
x_train, x_test, y_train, y_test = train_test_split(
    x_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Train Logistic Regression on full training data
logistic_model.fit(x_train, y_train)
y_pred = logistic_model.predict(x_test)

# Final Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
aucroc = roc_auc_score(y_test, y_pred)

print("\nðŸ“Š Final Model Evaluation (Test Set):")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"AUC-ROC: {aucroc:.4f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False,
            xticklabels=["Benign", "Malignant"], yticklabels=["Benign", "Malignant"])
plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

# Grid Search for Hyperparameter Tuning
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(x_train, y_train)

# Best Model Evaluation
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(x_test)
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)
aucroc_best = roc_auc_score(y_test, y_pred_best)

print("\nðŸ“Š Best Model Evaluation (Test Set):")
print(f"Accuracy: {accuracy_best:.4f}")
print(f"Precision: {precision_best:.4f}")
print(f"Recall: {recall_best:.4f}")
print(f"F1-Score: {f1_best:.4f}")
print(f"AUC-ROC: {aucroc_best:.4f}")

# ROC Curve Comparison
y_pred_proba_before = logistic_model.predict_proba(x_test)[:, 1]
y_pred_proba_after = best_model.predict_proba(x_test)[:, 1]
fpr_before, tpr_before, _ = roc_curve(y_test, y_pred_proba_before)
fpr_after, tpr_after, _ = roc_curve(y_test, y_pred_proba_after)

plt.figure(figsize=(10, 6))
plt.plot(fpr_before, tpr_before, label=f"Before Tuning (AUC = {auc(fpr_before, tpr_before):.4f})", color='blue')
plt.plot(fpr_after, tpr_after, label=f"After Tuning (AUC = {auc(fpr_after, tpr_after):.4f})", color='green')
plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Guess')
plt.title("ROC Curve Comparison")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.legend(loc="lower right")
plt.grid(alpha=0.5)
plt.show()

# Bar Plot for Comparison
models = ['Before Tuning', 'After Tuning']
accuracy_scores = [accuracy, accuracy_best]
precision_scores = [precision, precision_best]
recall_scores = [recall, recall_best]
f1_scores = [f1, f1_best]
aucroc_scores = [aucroc, aucroc_best]

bar_width = 0.15
index = np.arange(len(models))

plt.figure(figsize=(12, 6))
plt.bar(index, aucroc_scores, bar_width, label='AUC-ROC', color='cornflowerblue')
plt.bar(index + bar_width, accuracy_scores, bar_width, label='Accuracy', color='lightseagreen')
plt.bar(index + 2 * bar_width, precision_scores, bar_width, label='Precision', color='lightcoral')
plt.bar(index + 3 * bar_width, recall_scores, bar_width, label='Recall', color='gold')
plt.bar(index + 4 * bar_width, f1_scores, bar_width, label='F1-Score', color='orchid')
plt.xticks(index + 2 * bar_width, models)
plt.ylabel("Scores")
plt.title("Performance Comparison Before and After Tuning")
plt.legend()
plt.tight_layout()
plt.show()